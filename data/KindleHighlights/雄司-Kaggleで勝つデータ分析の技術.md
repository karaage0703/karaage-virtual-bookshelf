---
kindle-sync:
  bookId: '51195'
  title: Kaggleで勝つデータ分析の技術
  author: 門脇 大輔、阪田 隆司、保坂 桂佑、平松 雄司
  asin: B07YTDBC3Z
  lastAnnotatedDate: '2025-09-22'
  bookImageUrl: 'https://m.media-amazon.com/images/I/71kBM0KZSAL._SY160.jpg'
  highlightsCount: 10
---
# Kaggleで勝つデータ分析の技術
## Metadata
* Author: [門脇 大輔、阪田 隆司、保坂 桂佑、平松 雄司](https://www.amazon.comundefined)
* ASIN: B07YTDBC3Z
* Reference: https://www.amazon.com/dp/B07YTDBC3Z
* [Kindle link](kindle://book?action=open&asin=B07YTDBC3Z)

## Highlights
分析コンペに参加したり、上位入賞することで実力を示せば、データサイエンティストとしての就業機会を得やすくなるでしょう。2018年頃から国内でもKagglerを積極的に採用しようとする企業が現れてきています。 — location: [1008](kindle://book?action=open&asin=B07YTDBC3Z&location=1008) ^ref-45308

---
Exploratory Data Analysis, EDA） — location: [1072](kindle://book?action=open&asin=B07YTDBC3Z&location=1072) ^ref-39614

---
テーブルデータのコンペにおいて特徴量の作成は非常に重要な要素で、良い特徴量を作れたかどうかで順位が決まることがほとんどです。 — location: [1102](kindle://book?action=open&asin=B07YTDBC3Z&location=1102) ^ref-25832

---
このコンペの評価指標はaccuracyですが、accuracyは小さな改善をとらえづらいので、合わせてloglossという指標も出力するようにします。loglossは予測確率が外れているほど高いペナルティが与えられ、低いほど良い指標です。loglossについて詳しくは「2.3.4 二値分類における評価指標～正例である確率を予測値とする場合」を参照してください。 — location: [1138](kindle://book?action=open&asin=B07YTDBC3Z&location=1138) ^ref-16317

---
RMSE（Root Mean Squared Error：平均平方二乗誤差） — location: [1397](kindle://book?action=open&asin=B07YTDBC3Z&location=1397) ^ref-43816

---
precisionは、正例と予測したもののうち真の値も正例の割合、recallは真の値が正例のもののうちどの程度を正例の予測として含めることができているかの割合 — location: [1500](kindle://book?action=open&asin=B07YTDBC3Z&location=1500) ^ref-20018

---
評価指標と目的関数の違い — location: [1736](kindle://book?action=open&asin=B07YTDBC3Z&location=1736) ^ref-16680

---
分析コンペでモデルの精度を上げるために最も重要な要素である、特徴量の作成方法について説明します。分析コンペごとに有効な特徴量は異なり、あるコンペで効いた手法が他のコンペでは効かないことはよくあります。また、データの性質から効くに違いないと思った特徴量が作ってみたら効かないこともよくあります。そのため、さまざまな特徴量を作り試してみることが重要で、その参考となるようにさまざまな手法や観点を紹介します。 — location: [2121](kindle://book?action=open&asin=B07YTDBC3Z&location=2121) ^ref-34605

---
「決定木の気持ちになって考える」というフレーズがKagglerの間で使われることがあります。このフレーズは、どのような特徴量が有効かについて、Kagglerの感覚をよく表現しています。 — location: [2187](kindle://book?action=open&asin=B07YTDBC3Z&location=2187) ^ref-37021

---
学習時にモデルが複雑なときに罰則を科すことを正則化と言います。正則化により、罰則を上回るほど予測に寄与する場合のみモデルが複雑になるため、過学習を抑えることができます。 — location: [4058](kindle://book?action=open&asin=B07YTDBC3Z&location=4058) ^ref-32468

---
